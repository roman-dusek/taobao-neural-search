{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_gram_1 = torch.rand(10,5,20)\n",
    "q_gram_2 = torch.rand(10,5,20)\n",
    "q_seq = torch.rand(10,5,20)\n",
    "q_his = torch.rand(10,5,20)\n",
    "\n",
    "query_sources = [q_gram_1, q_gram_2, q_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGrainSemanticUnit(nn.Module):\n",
    "\n",
    "    def __init__(self, hiddne_dim=20):\n",
    "        super().__init__()\n",
    "        self.dp_attn = DotProductAttention()\n",
    "        self.trm = nn.TransformerEncoderLayer(d_model=hiddne_dim, nhead=4, dim_feedforward=hiddne_dim)\n",
    "    \n",
    "    def forward(self, query_sources, q_his):\n",
    "        \"\"\"\n",
    "        query_sources is expected to be [q_gram_1, q_gram_2, q_seq]\n",
    "        \"\"\"\n",
    "\n",
    "        pooled_sources = []\n",
    "        for query_source in query_sources:\n",
    "            pooled_sources.append(torch.mean(query_source, dim=1).unsqueeze(1))\n",
    "\n",
    "        \n",
    "        q_seq_his = self.dp_attn(pooled_sources[-1], q_his, q_his.shape[0])\n",
    "        q_seq_seq = torch.mean(self.trm(q_seq),1, keepdim=True)\n",
    "        q_mix = q_seq_his + q_seq_seq + torch.sum(torch.cat(pooled_sources,1),1, keepdim=True)\n",
    "        q_msg = torch.cat([*pooled_sources, q_seq_seq, q_seq_his, q_mix],2)\n",
    "\n",
    "        return q_msg\n",
    "\n",
    "mgs = MultiGrainSemanticUnit()\n",
    "\n",
    "q_msg = mgs([q_gram_1,q_gram_2,q_seq],q_his)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dp_attn = DotProductAttention()\n",
    "        \n",
    "    def forward(self, q_msg, user_features):\n",
    "        q_msg = q_msg.view(q_msg.shape[0],6,-1)\n",
    "\n",
    "        h_reprs = []\n",
    "\n",
    "        for q_repr_idx in range(q_msg.shape[1]):\n",
    "            h_reprs.append(self.dp_attn(q_msg[:,q_repr_idx].unsqueeze(1), user_features, user_features.shape[0]))\n",
    "        return torch.cat(h_reprs,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Optional, Callable\n",
    "\n",
    "class MLP(torch.nn.Sequential):\n",
    "    \"\"\"This block implements the multi-layer perceptron (MLP) module.\n",
    "    Args:\n",
    "        in_channels (int): Number of channels of the input\n",
    "        hidden_channels (List[int]): List of the hidden channel dimensions\n",
    "        norm_layer (Callable[..., torch.nn.Module], optional): Norm layer that will be stacked on top of the convolution layer. If ``None`` this layer wont be used. Default: ``None``\n",
    "        activation_layer (Callable[..., torch.nn.Module], optional): Activation function which will be stacked on top of the normalization layer (if not None), otherwise on top of the conv layer. If ``None`` this layer wont be used. Default: ``torch.nn.ReLU``\n",
    "        inplace (bool): Parameter for the activation layer, which can optionally do the operation in-place. Default ``True``\n",
    "        bias (bool): Whether to use bias in the linear layer. Default ``True``\n",
    "        dropout (float): The probability for the dropout layer. Default: 0.0\n",
    "\n",
    "    https://github.com/pytorch/vision/blob/ce257ef78b9da0430a47d387b8e6b175ebaf94ce/torchvision/ops/misc.py#L263\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        hidden_channels: List[int],\n",
    "        norm_layer: Optional[Callable[..., torch.nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., torch.nn.Module]] = torch.nn.ReLU,\n",
    "        inplace: Optional[bool] = True,\n",
    "        bias: bool = True,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        params = {} if inplace is None else {\"inplace\": inplace}\n",
    "\n",
    "        layers = []\n",
    "        in_dim = in_channels\n",
    "        for hidden_dim in hidden_channels[:-1]:\n",
    "            layers.append(torch.nn.Linear(in_dim, hidden_dim, bias=bias))\n",
    "            if norm_layer is not None:\n",
    "                layers.append(norm_layer(hidden_dim))\n",
    "            layers.append(activation_layer(**params))\n",
    "            layers.append(torch.nn.Dropout(dropout, **params))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(torch.nn.Linear(in_dim, hidden_channels[-1], bias=bias))\n",
    "        layers.append(torch.nn.Dropout(dropout, **params))\n",
    "\n",
    "        super().__init__(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "\n",
    "    def __init__(self,hidden_dim = 20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.msgu = MultiGrainSemanticUnit()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.mh_attn = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "\n",
    "        self.u_dp_attn = UserDotProductAttention()\n",
    "\n",
    "        self.cls_token = nn.Embedding(1,20)\n",
    "\n",
    "    def forward(self, query_feautures, user_features):\n",
    "        \"\"\"\n",
    "        query_feautures is expected to be  [[q_gram_1, q_gram_2, q_seq],q_his]\n",
    "        user_features = is expected to be [real, short, long]-time features\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(query_feautures) == 2 and len(query_feautures[0]) == 3\n",
    "\n",
    "        q_msg = self.msgu(query_feautures[0], query_feautures[1])\n",
    "\n",
    "        real, short, long = user_features\n",
    "\n",
    "        real_part = self.real_time_part(q_msg, real)\n",
    "        short_part = self.short_time_part(q_msg, short)\n",
    "        long_part = self.long_time_part(q_msg, long)\n",
    "\n",
    "        cls_token = self.cls_token(torch.tensor([[0]])).expand(long_part.shape[0],1, -1)\n",
    "\n",
    "        sequence = torch.cat([cls_token, q_msg.view(-1,6, self.hidden_dim), real_part, short_part, long_part], 1)\n",
    "\n",
    "        x, _ = self.mh_attn(sequence,sequence,sequence)\n",
    "\n",
    "        return x[:,0]\n",
    "        \n",
    "\n",
    "    def real_time_part(self, q_msg, user_real_features):\n",
    "        x, (hn, cn) = self.lstm(user_real_time)\n",
    "\n",
    "        x, _ = self.mh_attn(x, x, x)\n",
    "        x = torch.cat([torch.zeros(x.shape[0], 1, self.hidden_dim),x], dim=1)\n",
    "        return torch.cat(self.u_dp_attn(q_msg, x),1)\n",
    "\n",
    "    def short_time_part(self,q_msg, user_short_features):\n",
    "        x, _ = self.mh_attn(user_short_features, user_short_features, user_short_features)\n",
    "        x = torch.cat([torch.zeros(x.shape[0], 1, self.hidden_dim),x], dim=1)\n",
    "        return torch.cat(self.u_dp_attn(q_msg, x),1)\n",
    "\n",
    "    def long_time_part(self, q_msg, user_long_features):\n",
    "        \"\"\"\n",
    "        user_long_features is expected to be [4x[clicked, bought, collected]]\n",
    "        \"\"\"\n",
    "        h_attributes=[]\n",
    "        for attribute_index in range(user_long_features.shape[1]):\n",
    "            x = torch.mean(user_long_features[:,attribute_index], dim = 2)\n",
    "            x = torch.cat([torch.zeros(x.shape[0], 1, self.hidden_dim),x], dim=1)\n",
    "            h_attributes.append(torch.cat(self.u_dp_attn(q_msg, x),1))\n",
    "        return torch.cat(h_attributes,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_real_time = torch.rand(10,3,20)  # [bs, items, dim]\n",
    "user_short_time = torch.rand(10,7,20)  # [bs, items, dim]\n",
    "user_long_time = torch.rand(10,4,3,5,20)  # [bs, attributes, interactions, items, dim]\n",
    "\n",
    "user_tower = UserTower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user_emb \u001b[38;5;241m=\u001b[39m \u001b[43muser_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_sources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_his\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43muser_real_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_short_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_long_time\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/taobao-neural-search/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [13], line 28\u001b[0m, in \u001b[0;36mUserTower.forward\u001b[0;34m(self, query_feautures, user_features)\u001b[0m\n\u001b[1;32m     24\u001b[0m q_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsgu(query_feautures[\u001b[38;5;241m0\u001b[39m], query_feautures[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     26\u001b[0m real, short, long \u001b[38;5;241m=\u001b[39m user_features\n\u001b[0;32m---> 28\u001b[0m real_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal_time_part\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m short_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshort_time_part(q_msg, short)\n\u001b[1;32m     30\u001b[0m long_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_time_part(q_msg, long)\n",
      "Cell \u001b[0;32mIn [13], line 46\u001b[0m, in \u001b[0;36mUserTower.real_time_part\u001b[0;34m(self, q_msg, user_real_features)\u001b[0m\n\u001b[1;32m     44\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmh_attn(x, x, x)\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim),x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu_dp_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "user_emb = user_tower([query_sources, q_his], [user_real_time, user_short_time, user_long_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTower(nn.Module):\n",
    "\n",
    "    def __init__(self, num_item_emb, num_title_emb, hidden_dim = 20):\n",
    "        super().__init__()\n",
    "        self.item_id_emb = nn.Embedding(num_item_emb, hidden_dim)\n",
    "        self.title_emb = nn.EmbeddingBag(num_title_emb, hidden_dim)\n",
    "\n",
    "        self.mlp = MLP(hidden_dim, [hidden_dim])\n",
    "\n",
    "    def forward(self, item_index, title_indexes):\n",
    "\n",
    "        item_e = self.item_id_emb(item_index.flatten())\n",
    "        title_e = self.title_emb(title_indexes)\n",
    "\n",
    "        return item_e + torch.tanh(self.mlp(title_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_indices = torch.randint(0,100, (10,1))\n",
    "titles_indices = torch.randint(0,100, (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_tower = ItemTower(100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_emb = item_tower(item_indices,titles_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [549], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43m(\u001b[49m\u001b[43muser_emb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitem_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(user_emb @ item_emb.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12f2a913bebbea7c5e42ddf6f6351e572e373245b324ec9ca2bb51058b3cd7b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
