{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_gram_1 = torch.rand(10,5,20)\n",
    "q_gram_2 = torch.rand(10,5,20)\n",
    "q_seq = torch.rand(10,5,20)\n",
    "q_his = torch.rand(10,5,20)\n",
    "\n",
    "query_sources = [q_gram_1, q_gram_2, q_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, pooled_q_seq, q_his, batch_size):\n",
    "        return torch.mean(torch.softmax(torch.bmm(pooled_q_seq, q_his.permute(0,2,1)), -1).view(batch_size,-1,1) * q_his,1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGrainSemanticUnit(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        dp_attn = DotProductAttention()\n",
    "        trm = nn.TransformerEncoderLayer(d_model=20, nhead=4, dim_feedforward=20)\n",
    "    \n",
    "    def forward(self, query_sources, q_his):\n",
    "        \"\"\"\n",
    "        query_sources is expected to be [q_gram_1, q_gram_2, q_seq]\n",
    "        \"\"\"\n",
    "\n",
    "        pooled_sources = []\n",
    "        for query_source in query_sources:\n",
    "            pooled_sources.append(torch.mean(query_source, dim=1).unsqueeze(1))\n",
    "\n",
    "        \n",
    "        q_seq_his = dp_attn(pooled_sources[-1], q_his, q_his.shape[0])\n",
    "        q_seq_seq = torch.mean(trm(q_seq),1, keepdim=True)\n",
    "        q_mix = q_seq_his + q_seq_seq + torch.sum(torch.cat(pooled_sources,1),1, keepdim=True)\n",
    "        q_msg = torch.cat([*pooled_sources, q_seq_seq, q_seq_his, q_mix],2)\n",
    "\n",
    "        return q_msg\n",
    "\n",
    "mgs = MultiGrainSemanticUnit()\n",
    "\n",
    "q_msg = mgs([q_gram_1,q_gram_2,q_seq],q_his)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_real_time = torch.rand(10,3,20)\n",
    "user_short_time = torch.rand(10,7,20)\n",
    "user_long_time = torch.rand(10,14,20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 20])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_msg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dp_attn = DotProductAttention()\n",
    "        \n",
    "    def forward(self, q_msg, user_features):\n",
    "        q_msg = q_msg.view(q_msg.shape[0],6,-1)\n",
    "\n",
    "        h_reprs = []\n",
    "\n",
    "        for q_repr_idx in range(q_msg.shape[1]):\n",
    "            h_reprs.append(self.dp_attn(q_msg[:,q_repr_idx].unsqueeze(1), user_features, user_features.shape[0]))\n",
    "        return torch.cat(h_reprs,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_msg = q_msg.view(q_msg.shape[0],6,-1)\n",
    "\n",
    "h_short_representations = []\n",
    "\n",
    "for q_representation_idx in range(q_msg.shape[1]):\n",
    "    h_short_representations.append(dp_attn(q_msg[:,q_representation_idx].unsqueeze(1), user_short_time, user_short_time.shape[0]))\n",
    "\n",
    "torch.cat(h_short_representations,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTower(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.msgu = MultiGrainSemanticUnit()\n",
    "\n",
    "        self.lstm = nn.LSTM(input=20, hidden_size=20, n_layers=1, batch_first=True)\n",
    "        self.mh_attn = nn.MultiheadAttention(20, 4, batch_first=True)\n",
    "\n",
    "        self.u_dp_attn = UserDotProductAttention()\n",
    "\n",
    "    def forward(self, query_feautures, user_features):\n",
    "        \"\"\"\n",
    "        query_feautures is expected to be  [[q_gram_1, q_gram_2, q_seq],q_his]\n",
    "        user_features = is expected to be [real, short, long]-time features\n",
    "        \"\"\"\n",
    "\n",
    "        assert query_feautures.shape[0] == 2 and query_feautures[0].shape[0] == 3\n",
    "\n",
    "        q_msg = self.msgu(query_feautures[0], query_feautures[1])\n",
    "\n",
    "        real, short, long = user_features\n",
    "\n",
    "        self.real_time_part(q_msg, real)\n",
    "        self.short_time_part(q_msg, short)\n",
    "        self.long_time_part(q_msg, long)\n",
    "        \n",
    "\n",
    "    def real_time_part(self, q_msg, user_real_features):\n",
    "        x = self.lstm(user_real_features)\n",
    "        x= self.mh_attn(x)\n",
    "        x = torch.cat([torch.zeros(x.shape[0], 1, 20),x], dim=1)\n",
    "        return self.u_dp_attn(q_msg, x)\n",
    "\n",
    "    def short_time_part(self, user_short_features):\n",
    "        x= self.mh_attn(x)\n",
    "        x = torch.cat([torch.zeros(x.shape[0], 1, 20),x], dim=1)\n",
    "        return self.u_dp_attn(q_msg, x)\n",
    "\n",
    "    def long_time_part(self, user_long_features):\n",
    "        x= self.mh_attn(x)\n",
    "        x = torch.cat([torch.zeros(x.shape[0], 1, 20),x], dim=1)\n",
    "        return self.u_dp_attn(q_msg, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTower(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        item_id_emb = nn.Embedding()\n",
    "        title_emb = nn.EmbeddingBag()\n",
    "\n",
    "    def forward(self, item_id, title):\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "12f2a913bebbea7c5e42ddf6f6351e572e373245b324ec9ca2bb51058b3cd7b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
